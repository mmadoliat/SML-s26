[
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for MSSC 6250 - Statistical Machine Learning by Dr.Â Mehdi Maadooliat in Spring 2026 at Marquette University. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#class-meetings",
    "href": "course-overview.html#class-meetings",
    "title": "Course overview",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nCU 208\nTue & Thur 12:30 pm - 1:45 pm\n\n\nOffice Hours\nCU 351\nTue & Thur 1:45 - 3:15 pm",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Machine Learning - Spring 2026",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nTOPIC\nMATERIALS\nCODE\nDUE\n\n\n\n\n1\nTue, Jan 13\n[ğŸ¥ Lecture 1] Introduction\nğŸ”—Syllabus\n\n\n\n\n\n\n\n\nThu, Jan 15\n[ğŸ¥ Lecture 2] Review Python\nğŸ“šChapt 1\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nFri, Jan 16\nMore on Python\nğŸ’»[More Python Review]  ğŸ“–[here and there]\n\n\nğŸ“ HW 0 or  Learn R in R\n\n\n2\nTue, Jan 20\n[ğŸ¥ Lecture 3] Linear Algebra\nğŸ“šReview LA\n\n\n\n\n\n\n\n\nThu, Jan 22\n[ğŸ¥ Lecture 4] Probability Distributions (Normal)\nğŸ“šReview PD\n\n\n\n\n\n\n3\nTue, Jan 27\n[ğŸ¥ Lecture 5] What is SML?\nğŸ“šChapt 2-A\n\n\n\n\n\n\n\n\nThu, Jan 29\n[ğŸ¥ Lecture 6] Bias and Variance Trade-off\nğŸ“š[Chapt 2-B]\n\n\n\n\n\n\n\n\nFri, Jan 30\n\n\n\n\nD2Lâœï¸\nğŸ“ HW 1 at 11:50 pm  Solution\n\n\n4\nTue, Feb 3\n[ğŸ¥ Lecture 7] Basics of Linear Regression\nğŸ“šChapt 3-A\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Feb 5\n[ğŸ¥ Lecture 8] Wrap-up Linear Regression\nğŸ“š[Chapt 3-B]\n\n\n\n\n\n\n5\nTue, Feb 10\n[ğŸ¥ Lecture 9] Logistic Regression\nğŸ“šChapt 4-A\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Feb 12\n[ğŸ¥ Lecture 10] Linearâ€¯Discriminant Analysis and ROC\nğŸ“š[Chapt 4-B]\n\n\n\n\n\n\n\n\nFri, Feb 13\n\n\n\n\nD2Lâœï¸\nğŸ“ HW 2 at 11:50 pm  Solution\n\n\n6\nTue, Feb 17\n[ğŸ¥ Lecture 11] Quadratic Discriminant Analysis and Naive Bayes\nğŸ“š[Chapt 4-C]\n\n\n\n\n\n\n\n\nThu, Feb 19\n[ğŸ¥ Lecture 12] Resampling (Cross-validation)\nğŸ“šChapt 5\nRğŸ‘©â€ğŸ’»\n\n\n\n\n7\nTue, Feb 24\n[ğŸ¥ Lecture 13] Model Selection\nğŸ“šChapt 6-A\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Feb 26\n[ğŸ¥ Lecture 14] Ridge Regression\nğŸ“š[Chapt 6-B]\n\n\n\n\n\n\n\n\nFri, Feb 27\n\n\n\n\nD2Lâœï¸\nğŸ“ HW 3 at 11:50 pm  Solution\n\n\n8\nTue, Mar 3\n[ğŸ¥ Lecture 15] Lasso / Review for Exam\nğŸ“š[Chapt 6-C]\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Mar 5\n[ğŸ¥ Lecture 16] Exam 1\nğŸ“–[Exam 1]\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nFri, Mar 6\n\n\n\n\nD2Lâœï¸\nğŸ“ HW 4 at 11:50 pm  Solution\n\n\n\n\nTue, Mar 10\nğŸŒ´ Spring Break\n\n\n\n\n\n\n\n\n\n\nThu, Mar 12\nğŸŒ´ Spring Break\n\n\n\n\n\n\n\n\n9\nTue, Mar 17\n[ğŸ¥ Lecture 17] Guest Lecture by Dr.Â Rowe\nğŸ“š[Guest Lecture]\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Mar 19\n[ğŸ¥ Lecture 18] Smoothing:â€¯Basis functions (B-spline)\nğŸ“šChapt 7-A\n\n\n\n\n\n\n10\nTue, Mar 24\n[ğŸ¥ Lecture 19] More on Smoothing\nğŸ“š[Chapt 7-B]\n\n\n\n\n\n\n\n\nThu, Mar 26\n[ğŸ¥ Lecture 20] Basics of PCA and ICA\nğŸ“š[Chapt 7-C]\n\n\n\n\n\n\n\n\nFri, Mar 27\n\n\n\n\nD2Lâœï¸\nğŸ“ HW 5 at 11:50 pm  Solution\n\n\n11\nTue, Mar 31\n[ğŸ¥ Lecture 21] Basics of SVM\nğŸ“šChapt 9-A\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Apr 2\nğŸ° Easter Break\n\n\n\n\n\n\n\n\n\n\nFri, Apr 3\n\n\n\n\nD2Lâœï¸\nğŸ“ HW 6 at 11:50 pm  Solution\n\n\n12\nTue, Apr 7\n[ğŸ¥ Lecture 22] More on SVM\nğŸ“š[Chapt 9-B]\n\n\n\n\n\n\n\n\nThu, Apr 9\n[ğŸ¥ Lecture 23] Basics ofâ€¯Clustering (K-means)\nğŸ“šChapt 12-A\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nFri, Apr 10\n\n\n\n\n[D2Lâœï¸]\nğŸ“ [HW 7] at 11:50 pm\n\n\n13\nTue, Apr 14\n[ğŸ¥ Lecture 24] More on Clustering (Hierarchical Clustering)\nğŸ“š[Chapt 12-B]\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Apr 16\n[ğŸ¥ Lecture 25] More on Clustering (PCA)\nğŸ“šChapt 12-C\n\n\n\n\n\n\n14\nTue, Apr 21\n[ğŸ¥ Lecture 26] Basics ofâ€¯Deep Learning\nğŸ“šChapt 10\nRğŸ‘©â€ğŸ’»\n\n\n\n\n\n\nThu, Apr 23\n[ğŸ¥ Lecture 27] More on Deep Learning\nğŸ’»[DLiR-A]\n\n\n\n\n\n\n15\nTue, Apr 28\n[ğŸ¥ Lecture 28] SDSS\nğŸ’»[DLiR-B]\n\n\n\n\n\n\n\n\nThu, Apr 30\n[ğŸ¥ Lecture 29] SDSS\nğŸ’»[DLiR-C]\n\n\n\n\n\n\n16\nTue, May 5\nFinal Project\nğŸ“ƒ25 MIN Presentations : 10:30 am - 12:30 pm  Evaluation - Group A  Evaluation - Group B  Evaluation - Group C  Evaluation - Group D",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus - Computational Probability",
    "section": "",
    "text": "Course Title: MSSC 6250: Statistical Machine Learning\nMeeting Time: TuTh 12:30pm - 1:45pm\nLocation: Cudahy Hall 208 (Microsoft Teams)\nWebsite: http://tinyurl.com/SML-MU",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus - Computational Probability",
    "section": "",
    "text": "Course Title: MSSC 6250: Statistical Machine Learning\nMeeting Time: TuTh 12:30pm - 1:45pm\nLocation: Cudahy Hall 208 (Microsoft Teams)\nWebsite: http://tinyurl.com/SML-MU",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#instructor-details",
    "href": "course-syllabus.html#instructor-details",
    "title": "Syllabus - Computational Probability",
    "section": "Instructor Details",
    "text": "Instructor Details\n\nName: Mehdi Maadooliat, Ph.D.\nOffice: CU 351\nOffice Hours: TTh (CU 351) 1:45pm - 3:15pm or by e-mail",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus - Computational Probability",
    "section": "Course Description",
    "text": "Course Description\nA modern course in probability. Foundations of probability for modeling random processes with computational techniques. Topics include counting techniques, probability of events, random variables, distribution functions, probability functions, probability density functions, expectation, moments, moment generating functions, special discrete and continuous distributions, sampling distributions, transformation of variables, prior and posterior distributions, Law of Large Numbers, Central Limit Theorem, the Bayesian paradigm. Numerical and computational methods will be covered throughout topics.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-outcomes",
    "href": "course-syllabus.html#learning-outcomes",
    "title": "Syllabus - Computational Probability",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the course, students will:\n\nUnderstand and Apply Fundamental Probability Concepts\nAnalyze and Model Random Processes\nPerform Variable Transformations\nImplement Computational Techniques\nInterpret Statistical Theorems\nCritically Evaluate Probabilistic Models\nCommunicate Probabilistic Findings",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus - Computational Probability",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nThree semesters of mathematics beyond calculus and MATH 4720 or equiv.\nPreferable knowledge is MSSC 5700 and MSSC 5710.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus - Computational Probability",
    "section": "Textbooks",
    "text": "Textbooks\n\nProbability and Statistics with R, 2nd edition by Maria Dolores Ugarte, Ana F. Militino, Alan T. Arnholt, 2016. ISBN: 9781466504394.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-breakdown",
    "href": "course-syllabus.html#grading-breakdown",
    "title": "Syllabus - Computational Probability",
    "section": "Grading Breakdown",
    "text": "Grading Breakdown\n\nHomework: 30%\nProject: 10%\nMidterm Exam: 30%\nFinal Exam: 30%\n\n\nGrading Scale\n\n\n\nGrade\nRange\n\n\n\n\nA\n93.5 - 100%\n\n\nA-\n90- 93.49%\n\n\nB+\n86.5 - 89.99%\n\n\nB\n83.5 - 86.49%\n\n\nB-\n80 - 83.49%\n\n\nC+\n76.5 - 79.99%\n\n\nC\n73.5 - 76.49%\n\n\nC-\n70 - 73.49%\n\n\nD+\n66.5 - 69.99%\n\n\nD\n60 - 66.49%\n\n\nF\n&lt; 59.99%",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#exams",
    "href": "course-syllabus.html#exams",
    "title": "Syllabus - Computational Probability",
    "section": "Exams",
    "text": "Exams\nTentatively, there will be a midterm (in class) on Oct.Â 23rd, plus the final (in class or take home): Dec.Â 8th from 10:30 - 12:30pm.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#homework",
    "href": "course-syllabus.html#homework",
    "title": "Syllabus - Computational Probability",
    "section": "Homework",
    "text": "Homework\nHomework is required so that you get a better understanding of the material covered, plus it will help you to keep up. You will get a better understanding of the material if you discuss it with someone. However, you must submit YOUR OWN work to D2L website. Assignments are mostly due at 11:50pm (check for due dates in D2L).\nNO LATE HOMEWORK WILL BE ACCEPTED NOR WILL YOU BE ALLOWED TO MAKE UP MISSED HOMEWORK! Plan accordingly! It is better to submit something, even if it is incomplete. You need to type your homework (preferably using LaTeX, or Quarto) and submit it as a PDF file. Scanned homework will be graded out of 80. Low quality scanned homework will be considered as NO submission.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#make-up-policy",
    "href": "course-syllabus.html#make-up-policy",
    "title": "Syllabus - Computational Probability",
    "section": "Make-up Policy",
    "text": "Make-up Policy\nThere will not be any make-up exam or homework unless there is an emergency.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#attendance",
    "href": "course-syllabus.html#attendance",
    "title": "Syllabus - Computational Probability",
    "section": "Attendance",
    "text": "Attendance\nAttendance is required and subject to the College of Arts and Sciences policy.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-honesty",
    "href": "course-syllabus.html#academic-honesty",
    "title": "Syllabus - Computational Probability",
    "section": "Academic Honesty",
    "text": "Academic Honesty\nStudents are expected to follow the Universityâ€™s policy on academic honesty as outlined in the Bulletin.\nTL;DR: Donâ€™t cheat!\nPlease abide by the following as you work on assignments in this course:\n\nCollaboration: Only work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must also be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss whatâ€™s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to questions (including any code) with anyone other than myself.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nOn individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\nOnline resources: I am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the courseâ€™s policy is that you may make use of any online resources (e.g., StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of generative artificial intelligence (AI): You should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitateâ€”rather than hinderâ€”learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nâœ… AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nâŒ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you.\n\nYou are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask the instructor.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus - Computational Probability",
    "section": "Important dates",
    "text": "Important dates\n\nMonday, August 25: Classes begin\nTuesday, September 2: Drop/add ends\nThursday - Friday, October 16 - 17: Fall Break\nFriday, November 14: Last day to withdraw with W\nThursday - Monday, November 26 - 30: Thanksgiving Holiday\nSaturday, December 6: Classes end\nMonday, December 8, 10:30 am - 12:30 pm: Presentations\n\nFor more important dates, see the full MU Academic Calendar.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-note",
    "href": "course-syllabus.html#important-note",
    "title": "Syllabus - Computational Probability",
    "section": "Important Note",
    "text": "Important Note\nThe syllabus may be modified throughout the course. Any substantial modifications will result in a reissued syllabus.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We havenâ€™t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Homework\nTI-84 Guideline\nStat Calculator\nDistribution Calculator\nStat Tables\nğŸ”— on D2L\nğŸ”— Download here\nğŸ”— StatCalc(JAMM)\nğŸ”— DistCalc\nğŸ”— StatTables\n\n\nAdvanced Texbooks\nğŸ”— R Package\nğŸ”— Advanced R by Hadley Wickham\nğŸ”— Quarto\nğŸ”— R Markdown\nğŸ”— ggplot2: Elegant Graphics for Data Analysis\nğŸ”— Fundamentals of Data Visualization\nğŸ”— Data Visualization: A Practical Introduction\nğŸ”— R for Data Science\n\n\nSome Package documentation\nğŸ”— ggplot2: ggplot2.tidyverse.org\nğŸ”— dplyr: dplyr.tidyverse.org\nğŸ”— tidyr: tidyr.tidyverse.org\nğŸ”— forcats: forcats.tidyverse.org\nğŸ”— stringr: stringr.tidyverse.org\nğŸ”— lubridate: lubridate.tidyverse.org\nğŸ”— readr: readr.tidyverse.org",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "slides/Topic7.html#classical-programming-vs-machine-learning",
    "href": "slides/Topic7.html#classical-programming-vs-machine-learning",
    "title": "Statistical Machine Learning",
    "section": "Classical Programming vs Machine Learning",
    "text": "Classical Programming vs Machine Learning\n\nDeep learning is often presented as algorithms that â€œwork like the brainâ€, that â€œthinkâ€ or â€œunderstandâ€.\n\n\n\n\nReality is however quite far from this dream\n\n\n\n\n\nAI: the effort to automate intellectual tasks normally performed by humans.\n\n\n\n\n\n\n\n\nML: Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? \n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine learning\n\n\nDeep learning"
  },
  {
    "objectID": "slides/Topic7.html#classical-programming-vs-machine-learning-1",
    "href": "slides/Topic7.html#classical-programming-vs-machine-learning-1",
    "title": "Statistical Machine Learning",
    "section": "Classical Programming vs Machine Learning",
    "text": "Classical Programming vs Machine Learning\n\nDeep learning is often presented as algorithms that â€œwork like the brainâ€, that â€œthinkâ€ or â€œunderstandâ€.\n\n\nÂ \n\n\n\nAI: the effort to automate intellectual tasks normally performed by humans.\n\n\n\n\n\n\nML: Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? \n\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine learning\n\n\nDeep learning"
  },
  {
    "objectID": "slides/Topic7.html#recipes-of-a-machine-learning-algorithm",
    "href": "slides/Topic7.html#recipes-of-a-machine-learning-algorithm",
    "title": "Statistical Machine Learning",
    "section": "Recipes of a Machine Learning Algorithm",
    "text": "Recipes of a Machine Learning Algorithm\n\n\nInput data points, e.g.Â \n\nIf the task is speech recognition, these data points could be sound files\nIf the task is image tagging, they could be picture files\n\nExamples of the expected output \n\nIn a speech-recognition task, these could be transcripts of sound files\nIn an image task, expected outputs could tags such as â€œdogâ€, â€œcatâ€, and so on\n\nA way to measure whether the algorithm is doing a good job \n\nThis is needed to determine the distance between the output and its expected output.\nThe measurement is used as a feedback signal to adjust the way the algorithm works."
  },
  {
    "objectID": "slides/Topic7.html#anatomy-of-a-neural-network",
    "href": "slides/Topic7.html#anatomy-of-a-neural-network",
    "title": "Statistical Machine Learning",
    "section": "Anatomy of a Neural Network",
    "text": "Anatomy of a Neural Network\n\n\n\n\n\nThe input data and corresponding targets \nLayers, which are combined into a network (or model) \nThe loss function, which provides feedback for learning \nThe optimizer, which determines how learning proceeds"
  },
  {
    "objectID": "slides/Topic7.html#lenet-5-a-pioneering-7-level-cnn",
    "href": "slides/Topic7.html#lenet-5-a-pioneering-7-level-cnn",
    "title": "Statistical Machine Learning",
    "section": "LeNet-5: a pioneering 7-level CNN",
    "text": "LeNet-5: a pioneering 7-level CNN\n\n\n\nThe first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of classifying handwritten digits.\nThe resulting network, dubbed LeNet, was used by the USPS in the 1990s to automate the reading of ZIP codes on mail envelopes.\nLeNet-5 was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images."
  },
  {
    "objectID": "slides/Topic7.html#why-30-years-gap",
    "href": "slides/Topic7.html#why-30-years-gap",
    "title": "Statistical Machine Learning",
    "section": "Why 30+ Years gap?",
    "text": "Why 30+ Years gap?\n\n\n\nIn 2011, Dan Ciresan from IDSIA (Switzerland) began to win academic image-classification competitions with GPU-trained deep neural networks\nin 2012, a team led by Alex Krizhevsky and advised by Geoffrey Hinton was able to achieve a top-five accuracy of 83.6%â€“a significant breakthrough (in 2011 it was only 74.3%). \nThree forces are driving advances in ML:\n\nHardware\nDatasets and benchmarks\nAlgorithmic advances"
  },
  {
    "objectID": "slides/Topic7.html#vgg16cnn-for-classification-and-detection",
    "href": "slides/Topic7.html#vgg16cnn-for-classification-and-detection",
    "title": "Statistical Machine Learning",
    "section": "VGG16â€“CNN for Classification and Detection",
    "text": "VGG16â€“CNN for Classification and Detection\n\n\n\nVGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford.\nThe model achieves 92.7% top-5 test accuracy in ImageNet. It was one of the famous model submitted to ILSVRC-2014.\nIt makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3Ã—3 kernel-sized filters one after another.\nVGG16 was trained for weeks using NVIDIA Titan Black GPUâ€™s."
  },
  {
    "objectID": "slides/Topic7.html#neural-network-parameters---activation-func.",
    "href": "slides/Topic7.html#neural-network-parameters---activation-func.",
    "title": "Statistical Machine Learning",
    "section": "Neural Network â€“ Parameters - Activation Func.",
    "text": "Neural Network â€“ Parameters - Activation Func.\n\n\n\n\n\nA Neural Network\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivation Function"
  },
  {
    "objectID": "slides/Topic7.html#linear-activation-function",
    "href": "slides/Topic7.html#linear-activation-function",
    "title": "Statistical Machine Learning",
    "section": "Linear Activation function",
    "text": "Linear Activation function\n\n\n\n\n\\(Z=\\color{green}{W_1}X+\\color{lightblue}{b_1}\\)\n\n\n\n\n\n\n\n\n\\(Y=\\color{red}{W_2}Z+\\color{blue}{b_2}\\)\n\n\n\n\n\\(Y=\\color{red}{W_2}\\{\\color{green}{W_1}X+\\color{lightblue}{b_1}\\}+\\color{blue}{b_2}\\)\n\n\n\n\n\\(Y=\\{\\color{red}{W_2}\\color{green}{W_1}\\}X+\\{\\color{red}{W_2}\\color{lightblue}{b_1}+\\color{blue}{b_2}\\}\\)\n\n\n\n\n\n\n\n\n\\(Y=\\color{red}{\\mathbf{W}^*}X+\\color{blue}{\\mathbf{b}^*}\\)\n\n\n\nHidden Layers Disappears"
  },
  {
    "objectID": "slides/Topic7.html#deep-learning-software",
    "href": "slides/Topic7.html#deep-learning-software",
    "title": "Statistical Machine Learning",
    "section": "Deep learning software",
    "text": "Deep learning software\n\n\n\n\nğŸ”— Wikipedia: Deep learning software"
  },
  {
    "objectID": "slides/Topic7.html#what-is-tensorflow",
    "href": "slides/Topic7.html#what-is-tensorflow",
    "title": "Statistical Machine Learning",
    "section": "What is TensorFlow?",
    "text": "What is TensorFlow?\n\n\nYou define the graph in R\nGraph is compiled and optimized\nGraph is executed on devices\nNodes represent computations\nData (tensors) flows between them"
  },
  {
    "objectID": "slides/Topic7.html#why-tensorflow-in-r",
    "href": "slides/Topic7.html#why-tensorflow-in-r",
    "title": "Statistical Machine Learning",
    "section": "Why TensorFlow in R?",
    "text": "Why TensorFlow in R?\n\n\nHardware independent\n\nCPU (via Eigen and BLAS)\nGPU (via CUDA and cuDNN)\nTPU (Tensor Processing Unit)\n\nSupports automatic differentiation\nDistributed execution and large datasets\nVery general built-in optimization algorithms (SGD, Adam) that donâ€™t require that all data is in RAM\nIt can be deployed with a low-latency C++ runtime\nR has a lot to offer as an interface language for TensorFlow"
  },
  {
    "objectID": "slides/Topic7.html#real-world-examples-of-data-tensors",
    "href": "slides/Topic7.html#real-world-examples-of-data-tensors",
    "title": "Statistical Machine Learning",
    "section": "Real-world examples of data tensors",
    "text": "Real-world examples of data tensors\n\n\n2D tensors\n\nVector dataâ€”(samples, features)  \n\n\n\n\n3D tensors\n\nGrayscale Imagesâ€”(samples, height, width)\nTime-series data or sequence dataâ€”(samples, timesteps, features)\n\n\n\n\n \n\n\n\n\n\n\n4D tensors\n\nColor Imagesâ€”(samples, height, width, channels)\n\n\n\n\n \n\n\n\n\n\n\n5D tensors\n\nVideoâ€”(samples, frames, height, width, channels)"
  },
  {
    "objectID": "slides/Topic7.html#why-keras",
    "href": "slides/Topic7.html#why-keras",
    "title": "Statistical Machine Learning",
    "section": "Why Keras?",
    "text": "Why Keras?\n\n\nIt allows the same code to run seamlessly on CPU or GPU.\nIt has a user-friendly API that makes it easy to quickly prototype deep-learning models."
  },
  {
    "objectID": "slides/Topic7.html#installing-keras",
    "href": "slides/Topic7.html#installing-keras",
    "title": "Statistical Machine Learning",
    "section": "Installing Keras",
    "text": "Installing Keras\nÂ  \n\n\n\n\nFirst, install the keras R package:\n\nremotes::install_github(\"rstudio/keras3\");    # OR\nInstall.packages(\"keras3\")\n\n\nTo install both the core Keras library as well as the TensorFlow backend\n\nlibrary(keras3)\nkeras3::install_keras(backend = \"tensorflow\")\n\n\n\nYou need Python installed before installing TensorFlow\n\nAnaconda (Python distribution), a free and open-source software\n\n\n\n\n\n\n\n\nYou can install TensorFlow with GPU support\n\nNVIDIAÂ® drivers,\nCUDA Toolkit v9.0, and\ncuDNN v7.0\n\n\n\n\n\nare needed: https://tensorflow.rstudio.com/tools/local_gpu.html"
  },
  {
    "objectID": "slides/Topic7.html#developing-a-deep-nn-with-keras",
    "href": "slides/Topic7.html#developing-a-deep-nn-with-keras",
    "title": "Statistical Machine Learning",
    "section": "Developing a Deep NN with Keras",
    "text": "Developing a Deep NN with Keras\nÂ  \n\n\n\n\nStep 1 - Define your training data:\n\ninput tensors and target tensors.\n\n\n\n\nStep 2 - Define a network of layers (or model)\n\nthat maps your inputs to your targets.\n\n\n\n\n\nStep 3 - Configure the learning process by choosing\n\na loss function,\nan optimizer,\nand some metrics to monitor.\n\n\n\n\n\nStep 4 - Iterate on your training data by calling the\n\nfit() method of your model."
  },
  {
    "objectID": "slides/Topic7.html#keras-step-1-data-preprocessing",
    "href": "slides/Topic7.html#keras-step-1-data-preprocessing",
    "title": "Statistical Machine Learning",
    "section": "Keras: Step 1 â€“ Data preprocessing",
    "text": "Keras: Step 1 â€“ Data preprocessing\nÂ  \n\n\n\n\nlibrary(keras3)\n\n# Load MNIST (Modified National Institute of Standards and Technology) images datasets\nc(c(x_train, y_train), c(x_test, y_test)) %&lt;-% dataset_mnist()\n\n# Flatten images and transform RGB values into [0,1] range \nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 784))\nx_test &lt;- array_reshape(x_test, c(nrow(x_test), 784))\nx_train &lt;- x_train / 255\nx_test &lt;- x_test / 255\n\n# Convert class vectors to binary class matrices\ny_train &lt;- to_categorical(y_train, 10)\ny_test &lt;- to_categorical(y_test, 10)"
  },
  {
    "objectID": "slides/Topic7.html#keras-step-2-model-definition",
    "href": "slides/Topic7.html#keras-step-2-model-definition",
    "title": "Statistical Machine Learning",
    "section": "Keras: Step 2 â€“ Model definition",
    "text": "Keras: Step 2 â€“ Model definition\nÂ  \n\n\n\n\n\nmodel &lt;- keras_model_sequential(input_shape = c(784)) \nmodel %&gt;% \n     layer_dense(units = 256, activation = 'relu') %&gt;% \n     layer_dropout(rate = 0.4) %&gt;% \n     layer_dense(units = 128, activation = 'relu') %&gt;%\n     layer_dropout(rate = 0.3) %&gt;%\n     layer_dense(units = 10, activation = 'softmax')\n     \nsummary(model)\n&gt;# Model: \"sequential\"\n&gt;# â”‚ Layer (type)          â”‚ Output Shape     â”‚ Param # â”‚\n&gt;# â”œ-----------------------â”¼------------------â”¼---------â”¤\n&gt;# â”‚ dense_11 (Dense)      â”‚ (None, 256)      â”‚ 200,960 â”‚\n&gt;# â”‚ dropout_3 (Dropout)   â”‚ (None, 256)      â”‚       0 â”‚\n&gt;# â”‚ dense_10 (Dense)      â”‚ (None, 128)      â”‚  32,896 â”‚\n&gt;# â”‚ dropout_2 (Dropout)   â”‚ (None, 128)      â”‚       0 â”‚\n&gt;# â”‚ dense_9 (Dense)       â”‚ (None, 10)       â”‚   1,290 â”‚\n&gt;# â””-----------------------â”´------------------â”´---------â”˜\n&gt;#  Total params: 235,146 (918.54 KB)\n&gt;#  Trainable params: 235,146 (918.54 KB)\n&gt;#  Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "slides/Topic7.html#multi-class-vs-multi-label-classification",
    "href": "slides/Topic7.html#multi-class-vs-multi-label-classification",
    "title": "Statistical Machine Learning",
    "section": "Multi-Class vs Multi-Label Classification",
    "text": "Multi-Class vs Multi-Label Classification"
  },
  {
    "objectID": "slides/Topic7.html#multi-class-vs-multi-label-classification-cont.",
    "href": "slides/Topic7.html#multi-class-vs-multi-label-classification-cont.",
    "title": "Statistical Machine Learning",
    "section": "Multi-Class vs Multi-Label Classification (Cont.)",
    "text": "Multi-Class vs Multi-Label Classification (Cont.)"
  },
  {
    "objectID": "slides/Topic7.html#multi-class-vs-multi-label-classification-cont.-1",
    "href": "slides/Topic7.html#multi-class-vs-multi-label-classification-cont.-1",
    "title": "Statistical Machine Learning",
    "section": "Multi-Class vs Multi-Label Classification (Cont.)",
    "text": "Multi-Class vs Multi-Label Classification (Cont.)"
  },
  {
    "objectID": "slides/Topic7.html#keras-step-3-compile-model",
    "href": "slides/Topic7.html#keras-step-3-compile-model",
    "title": "Statistical Machine Learning",
    "section": "Keras: Step 3 â€“ Compile Model",
    "text": "Keras: Step 3 â€“ Compile Model\nÂ  \n\n\n\n\nModel compilation prepares the model for training by:\n\nConverting the layers into a TensorFlow graph\nApplying the specified loss function and optimizer\nArranging for the collection of metrics during training\n\n\n\nmodel %&gt;% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)"
  },
  {
    "objectID": "slides/Topic7.html#keras-step-4-model-training",
    "href": "slides/Topic7.html#keras-step-4-model-training",
    "title": "Statistical Machine Learning",
    "section": "Keras: Step 4 â€“ Model Training",
    "text": "Keras: Step 4 â€“ Model Training\nÂ  \n\n\n\n\nUse the fit() to train the model for 10 epochs using batches of 128 images:\n\nFeed 128 samples at a time to the model (batch_size = 128)\nTraverse the input dataset 10 times (epochs = 10)\nHold out 20% of the data for validation (validation_split = 0.2)\n\n\n\nhistory &lt;- model %&gt;% fit(\n  x_train, y_train, \n  batch_size = 128, \n  epochs = 10,\n  validation_split = 0.2\n)\n\n\n\n\nEpoch 1/10\n375/375 â”â”â”â”â” 3s 5ms/step - accuracy: 0.7831 - loss: 0.6970 - val_accuracy: 0.9513 - val_loss: 0.1640\nEpoch 2/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9371 - loss: 0.2123 - val_accuracy: 0.9628 - val_loss: 0.1249\nEpoch 3/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9539 - loss: 0.1540 - val_accuracy: 0.9666 - val_loss: 0.1098\nEpoch 4/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9612 - loss: 0.1301 - val_accuracy: 0.9743 - val_loss: 0.0865\nEpoch 5/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9663 - loss: 0.1145 - val_accuracy: 0.9730 - val_loss: 0.0921\nEpoch 6/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9688 - loss: 0.1020 - val_accuracy: 0.9736 - val_loss: 0.0923\nEpoch 7/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9726 - loss: 0.0940 - val_accuracy: 0.9770 - val_loss: 0.0822\nEpoch 8/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9742 - loss: 0.0875 - val_accuracy: 0.9770 - val_loss: 0.0815\nEpoch 9/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9750 - loss: 0.0791 - val_accuracy: 0.9785 - val_loss: 0.0810\nEpoch 10/10\n375/375 â”â”â”â”â” 1s 3ms/step - accuracy: 0.9769 - loss: 0.0744 - val_accuracy: 0.9777 - val_loss: 0.0835\n\n\n\n\n\nmodel %&gt;% evaluate(x_test, y_test)\n&gt;# 313/313 â”â”â”â”â” 0s 771us/step - accuracy: 0.9747 - loss: 0.0930\n&gt;# $accuracy\n&gt;# [1] 0.9791\n&gt;# \n&gt;# $loss\n&gt;# [1] 0.0784568"
  },
  {
    "objectID": "slides/Topic7.html#keras-evaluation-and-prediction",
    "href": "slides/Topic7.html#keras-evaluation-and-prediction",
    "title": "Statistical Machine Learning",
    "section": "Keras: Evaluation and prediction",
    "text": "Keras: Evaluation and prediction\nÂ  \n\nplot(history)\n\n\nmodel %&gt;% predict(x_test[1:100,]) %&gt;% apply(1, which.max)-1\n&gt;# 4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 957us/step\n&gt;#   [1] 7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7\n&gt;#  [19] 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2\n&gt;#  [37] 7 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5\n&gt;#  [55] 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0\n&gt;#  [73] 2 9 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1\n&gt;#  [91] 3 6 9 3 1 4 1 7 6 9\n\nround(model %&gt;% predict(x_test[1:9,]),5)\n&gt;# 1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 16ms/step\n&gt;#          [,1]    [,2]  [,3]  [,4]    [,5]    [,6]    [,7]    [,8]  [,9]   [,10]\n&gt;#  [1,] 0.00000 0.00000 0e+00 0.000 0.00000 0.00000 0.00000 1.00000 0e+00 0.00000\n&gt;#  [2,] 0.00000 0.00000 1e+00 0.000 0.00000 0.00000 0.00000 0.00000 0e+00 0.00000\n&gt;#  [3,] 0.00000 0.99983 1e-05 0.000 0.00001 0.00000 0.00000 0.00014 1e-05 0.00000\n&gt;#  [4,] 0.99986 0.00000 6e-05 0.000 0.00000 0.00000 0.00007 0.00000 0e+00 0.00000\n&gt;#  [5,] 0.00000 0.00000 0e+00 0.000 0.99995 0.00000 0.00000 0.00000 0e+00 0.00005\n&gt;#  [6,] 0.00000 0.99998 0e+00 0.000 0.00000 0.00000 0.00000 0.00002 0e+00 0.00000\n&gt;#  [7,] 0.00000 0.00000 0e+00 0.000 0.99984 0.00000 0.00000 0.00000 3e-05 0.00013\n&gt;#  [8,] 0.00000 0.00001 1e-05 0.002 0.00007 0.00007 0.00000 0.00044 4e-05 0.99737\n&gt;#  [9,] 0.00000 0.00000 0e+00 0.000 0.00000 0.30770 0.69230 0.00000 0e+00 0.00000"
  },
  {
    "objectID": "slides/Topic7.html#keras-demo",
    "href": "slides/Topic7.html#keras-demo",
    "title": "Statistical Machine Learning",
    "section": "Keras Demo",
    "text": "Keras Demo\n\nkeras3.posit.co\nğŸ”— Launch R"
  },
  {
    "objectID": "slides/Topic7.html#keras-api-layers",
    "href": "slides/Topic7.html#keras-api-layers",
    "title": "Statistical Machine Learning",
    "section": "Keras API: Layers",
    "text": "Keras API: Layers\n\n90+ layers available (you can also create your own)\n\n\n\n\n\n\n\nlayer_dense(units = 64, kernel_regularizer = regularizer_l1(0.01))\nlayer_dense(units = 64, bias_regularizer = regularizer_l2(0.01))"
  },
  {
    "objectID": "slides/Topic7.html#embedding-layers",
    "href": "slides/Topic7.html#embedding-layers",
    "title": "Statistical Machine Learning",
    "section": "Embedding Layers",
    "text": "Embedding Layers\n\nVectorization of text that reflects semantic relationships between words\n\n \n\n\n\n\n\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_embedding(input_dim = 10000, output_dim = 8) %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n\n\n\nHow to use?\n\nLearn the embeddings jointly with the main task (e.g.Â classification); or\nLoad pre-trained word embeddings (e.g.Â Word2vec, GloVe)"
  },
  {
    "objectID": "slides/Topic7.html#hands-on-exercises",
    "href": "slides/Topic7.html#hands-on-exercises",
    "title": "Statistical Machine Learning",
    "section": "Hands-on Exercises",
    "text": "Hands-on Exercises\n\nBuild and train a CNN on Fashion MNIST\n\nMNIST CNN in keras3\n\n\n\n\n\n\n\nImplement an LSTM for\n\nText Classification in keras3\n\n\n\n\n\n\n\n\n\nCreate an autoencoder for dimensionality reduction and visualize embeddings"
  },
  {
    "objectID": "slides/Topic7.html#keras-for-r-cheatsheet",
    "href": "slides/Topic7.html#keras-for-r-cheatsheet",
    "title": "Statistical Machine Learning",
    "section": "Keras for R cheatsheet",
    "text": "Keras for R cheatsheet\n\n\n\n\n\n\n\n\n\n\nğŸ”— Rstudio: Keras cheatsheet"
  }
]